{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob, os, time\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "\n",
    "from metadec.dataset.genome import SimGenomeDataset\n",
    "from metadec.model.dec import DEC\n",
    "from metadec.utils.utils import load_genomics\n",
    "from metadec.debug.visualize import store_results\n",
    "from metadec.utils.metrics import genome_acc\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = '../results'\n",
    "GEN_DATA_DIR = '../data'\n",
    "\n",
    "# Versioning each runs\n",
    "ARCH = 'dec_genomics'\n",
    "DATE = '20200530'\n",
    "\n",
    "# Training batchsize\n",
    "BATCH_SIZE = 1\n",
    "# Maximum iterations for clustering optimization step\n",
    "MAX_ITERS = 10\n",
    "# Number of epochs for pretraining\n",
    "PRETRAIN_EPOCHS = 5\n",
    "# Interval for updating the training status\n",
    "UPDATE_INTERVAL = 1\n",
    "# Tolerance threshold to stop clustering optimization step\n",
    "TOL = 0.000001\n",
    "# Trained weight for pretrained autoencoder\n",
    "AE_WEIGHTS = None\n",
    "# Dir contains raw fasta data\n",
    "DATASET_DIR = GEN_DATA_DIR\n",
    "# Specifc dataset or all of them\n",
    "DATASET_NAME = 'S1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# Follows metaprob\n",
    "KMERS = [4]\n",
    "LMER = 30\n",
    "NUM_SHARED_READS = (5, 45)\n",
    "ONLY_SEED = True\n",
    "MAXIMUM_SEED_SIZE = 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = f'{RESULT_DIR}\\log'\n",
    "MODEL_DIR = f'{RESULT_DIR}\\model'\n",
    "META_INFO =  '../config/dataset_metadata.json'\n",
    "\n",
    "\n",
    "for d in [LOG_DIR, MODEL_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "# Dir for saving training results\n",
    "SAVE_DIR = os.path.join(MODEL_DIR, ARCH, DATE)\n",
    "\n",
    "# Dir for saving log results: visualization, training logs\n",
    "LOG_DIR = os.path.join(LOG_DIR, ARCH, DATE)\n",
    "\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(DATASET_DIR, 'raw')\n",
    "processed_dir = os.path.join(DATASET_DIR, 'processed')\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)\n",
    "if DATASET_NAME == 'all':\n",
    "    raw_datasets = glob.glob(raw_dir + '/*.fna')\n",
    "else:\n",
    "    raw_datasets = [os.path.join(raw_dir, DATASET_NAME + '.fna')]\n",
    "\n",
    "# Mapping of dataset and its corresponding number of clusters\n",
    "with open(META_INFO, 'r') as f:\n",
    "    n_clusters_mapping = json.load(f)['simulated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                                     \u001b[A\n",
      "  0%|                                                                          | 0/1 [07:04<?, ?it/s]\n",
      "                                                                                                     \u001b[A\n",
      "  0%|                                                                          | 0/1 [07:04<?, ?it/s]\n",
      "                                                                                                     \u001b[A\n",
      "  0%|                                                                          | 0/1 [07:04<?, ?it/s]\n",
      "  0%|                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset S1...\n",
      "Prior number of clusters: 2...\n",
      "Prior number of shared reads: 5...\n",
      "Reading fna file...\n",
      "Creating document from reads...\n",
      "Creating corpus...\n",
      "Deserializing data...\n",
      "Computing features...\n",
      "Serializing data to... ../data\\processed\\S1.json\n",
      "Normalizing...\n",
      "Finish.\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 1.0083\n",
      "        |==>  f1-score: 0.7005 <==|\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9892\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.9588\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.9268\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 0.8844\n",
      "Pretraining time: 9s\n",
      "Pretrained weights are saved to results\\model\\dec_genomics\\20200530\\S1/ae_weights.h5\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 136)               0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               68500     \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "encoder_latent (Dense)       (None, 10)                10010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 2)                 20        \n",
      "=================================================================\n",
      "Total params: 579,530\n",
      "Trainable params: 579,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Update interval 1\n",
      "Save interval 760\n",
      "Initializing cluster centers with k-means.\n",
      "Iter 0: precision = 0.98960, recall = 0.98960, f1_score = 0.98960,                            nmi = --, ari = --  ; loss= 0\n",
      "saving model to: results\\model\\dec_genomics\\20200530\\S1/DEC_model_0.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \n",
      "  0%|                                                                          | 0/1 [07:59<?, ?it/s]\n",
      "                                                                                                     \u001b[A\n",
      "  0%|                                                                          | 0/1 [07:59<?, ?it/s]\n",
      "  0%|                                                                          | 0/1 [00:55<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: precision = 0.98960, recall = 0.98960, f1_score = 0.98960,                            nmi = --, ari = --  ; loss= 0.04344\n",
      "delta_label  0.0 < tol  1e-06\n",
      "Reached tolerance threshold. Stopping training.\n",
      "saving model to: results\\model\\dec_genomics\\20200530\\S1/DEC_model_final.h5\n",
      "...\n",
      "Saving results...\n",
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 152 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 152 samples in 0.005s...\n",
      "[t-SNE] Computed conditional probabilities for sample 152 / 152\n",
      "[t-SNE] Mean sigma: 3.016403\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 72.761116\n",
      "[t-SNE] KL divergence after 400 iterations: 0.542474\n",
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 152 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 152 samples in 0.006s...\n",
      "[t-SNE] Computed conditional probabilities for sample 152 / 152\n",
      "[t-SNE] Mean sigma: 0.739312\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 54.782673\n",
      "[t-SNE] KL divergence after 400 iterations: 0.285013\n",
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 152 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 152 samples in 0.003s...\n",
      "[t-SNE] Computed conditional probabilities for sample 152 / 152\n",
      "[t-SNE] Mean sigma: 0.739312\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 55.727520\n",
      "[t-SNE] KL divergence after 400 iterations: 0.256768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \n",
      "  0%|                                                                          | 0/1 [08:03<?, ?it/s]\n",
      "  0%|                                                                          | 0/1 [00:58<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization results are saved in:  results\\log\\dec_genomics\\20200530\\S1\n",
      "Finish clustering for dataset S1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \n",
      "  0%|                                                                          | 0/1 [08:03<?, ?it/s]\n",
      "                                                                                                     \u001b[A\n",
      "  0%|                                                                          | 0/1 [08:03<?, ?it/s]\n",
      "  0%|                                                                          | 0/1 [00:59<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████| 1/1 [00:59<00:00, 59.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.9896022497327923\n",
      "Clustering time: (time.time() - t0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in tqdm.tqdm(raw_datasets):\n",
    "    # Get some parameters\n",
    "    dataset_name = os.path.basename(dataset).split('.fna')[0]\n",
    "    save_dir = os.path.join(SAVE_DIR, dataset_name)\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    num_shared_read = NUM_SHARED_READS[1] if 'R' in dataset_name else NUM_SHARED_READS[0]\n",
    "    is_deserialize = os.path.exists(os.path.join(processed_dir, dataset_name + '.json'))\n",
    "    n_clusters = n_clusters_mapping[dataset_name]\n",
    "    \n",
    "    # Read dataset\n",
    "    tqdm.tqdm.write(f'Processing dataset {dataset_name}...')\n",
    "    tqdm.tqdm.write(f'Prior number of clusters: {n_clusters}...')\n",
    "    tqdm.tqdm.write(f'Prior number of shared reads: {num_shared_read}...')\n",
    "\n",
    "    try:\n",
    "        seed_kmer_features, labels, groups, seeds = load_genomics(\n",
    "            dataset,\n",
    "            kmers=KMERS,\n",
    "            lmer=LMER,\n",
    "            maximum_seed_size=MAXIMUM_SEED_SIZE,\n",
    "            num_shared_reads=num_shared_read,\n",
    "            is_deserialize=is_deserialize,\n",
    "            is_serialize=~is_deserialize,\n",
    "            is_normalize=True,\n",
    "            only_seed=ONLY_SEED,\n",
    "            graph_file=os.path.join(processed_dir, dataset_name + '.json')\n",
    "        )\n",
    "    except:\n",
    "        seed_kmer_features, labels, groups, seeds = load_genomics(\n",
    "            dataset,\n",
    "            kmers=KMERS,\n",
    "            lmer=LMER,\n",
    "            maximum_seed_size=MAXIMUM_SEED_SIZE,\n",
    "            num_shared_reads=num_shared_read,\n",
    "            is_deserialize=False,\n",
    "            is_serialize=True,\n",
    "            is_normalize=True,\n",
    "            only_seed=ONLY_SEED,\n",
    "            graph_file=os.path.join(processed_dir, dataset_name + '.json')\n",
    "        )\n",
    "    \n",
    "    # continue\n",
    "    # Initialize model\n",
    "    init_lr = 0.1\n",
    "    init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                               distribution='uniform')  # [-limit, limit], limit=sqrt(1./fan_in)\n",
    "    pretrain_optimizer = SGD(lr=init_lr, momentum=0.9, decay=init_lr/PRETRAIN_EPOCHS)\n",
    "\n",
    "    dec = DEC(dims=[seed_kmer_features.shape[-1], 500, 1000, 10], n_clusters=n_clusters, init=init)\n",
    "    \n",
    "    # Start clustering\n",
    "    if AE_WEIGHTS is None:\n",
    "        dec.pretrain(x=seed_kmer_features, y=labels, grps=groups, optimizer=pretrain_optimizer,\n",
    "                epochs=PRETRAIN_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                save_dir=save_dir)\n",
    "    else:\n",
    "        dec.autoencoder.load_weights(AE_WEIGHTS)\n",
    "\n",
    "    dec.model.summary()\n",
    "    t0 = time.time()\n",
    "    optim_lr = 0.01\n",
    "\n",
    "    dec.compile(optimizer=SGD(optim_lr), loss='kld')\n",
    "    y_pred = dec.fit(x=seed_kmer_features, y=labels, grps=groups, n_clusters=n_clusters, tol=TOL, maxiter=MAX_ITERS, batch_size=BATCH_SIZE,\n",
    "                      update_interval=UPDATE_INTERVAL, save_dir=save_dir)\n",
    "    \n",
    "    tqdm.tqdm.write('...')\n",
    "    latent = dec.encoder.predict(seed_kmer_features)\n",
    "    y_pred = dec.predict(seed_kmer_features)\n",
    "\n",
    "    tqdm.tqdm.write('Saving results...')\n",
    "    store_results(groups, seed_kmer_features, latent, labels, y_pred,\n",
    "                  n_clusters, dataset_name, save_dir=os.path.join(LOG_DIR, dataset_name))\n",
    "    tqdm.tqdm.write(f'Finish clustering for dataset {dataset_name}.')\n",
    "    tqdm.tqdm.write(f'F1-score: {genome_acc(groups, y_pred, labels, n_clusters)[2]}')\n",
    "    tqdm.tqdm.write(f'Clustering time: (time.time() - t0)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
