{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob, os, time\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "\n",
    "from metadec.dataset.genome import SimGenomeDataset\n",
    "from metadec.model.dec import DEC\n",
    "from metadec.utils.utils import load_genomics\n",
    "from metadec.debug.visualize import store_results\n",
    "from metadec.utils.metrics import genome_acc\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = '../results'\n",
    "GEN_DATA_DIR = '../data'\n",
    "\n",
    "# Versioning each runs\n",
    "ARCH = 'dec_genomics'\n",
    "DATE = '20200530'\n",
    "\n",
    "# Training batchsize\n",
    "BATCH_SIZE = 1\n",
    "# Maximum iterations for clustering optimization step\n",
    "MAX_ITERS = 10\n",
    "# Number of epochs for pretraining\n",
    "PRETRAIN_EPOCHS = 5\n",
    "# Interval for updating the training status\n",
    "UPDATE_INTERVAL = 1\n",
    "# Tolerance threshold to stop clustering optimization step\n",
    "TOL = 0.000001\n",
    "# Trained weight for pretrained autoencoder\n",
    "AE_WEIGHTS = None\n",
    "# Dir contains raw fasta data\n",
    "DATASET_DIR = GEN_DATA_DIR\n",
    "# Specifc dataset or all of them\n",
    "DATASET_NAME = 'S1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# Follows metaprob\n",
    "KMERS = [4]\n",
    "LMER = 30\n",
    "NUM_SHARED_READS = (5, 45)\n",
    "ONLY_SEED = True\n",
    "MAXIMUM_SEED_SIZE = 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = f'{RESULT_DIR}\\log'\n",
    "MODEL_DIR = f'{RESULT_DIR}\\model'\n",
    "META_INFO =  '../config/dataset_metadata.json'\n",
    "\n",
    "\n",
    "for d in [LOG_DIR, MODEL_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "# Dir for saving training results\n",
    "SAVE_DIR = os.path.join(MODEL_DIR, ARCH, DATE)\n",
    "\n",
    "# Dir for saving log results: visualization, training logs\n",
    "LOG_DIR = os.path.join(LOG_DIR, ARCH, DATE)\n",
    "\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(DATASET_DIR, 'raw')\n",
    "processed_dir = os.path.join(DATASET_DIR, 'processed')\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)\n",
    "if DATASET_NAME == 'all':\n",
    "    raw_datasets = glob.glob(raw_dir + '/*.fna')\n",
    "else:\n",
    "    raw_datasets = [os.path.join(raw_dir, DATASET_NAME + '.fna')]\n",
    "\n",
    "# Mapping of dataset and its corresponding number of clusters\n",
    "with open(META_INFO, 'r') as f:\n",
    "    n_clusters_mapping = json.load(f)['simulated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in tqdm.tqdm(raw_datasets):\n",
    "    # Get some parameters\n",
    "    dataset_name = os.path.basename(dataset).split('.fna')[0]\n",
    "    save_dir = os.path.join(SAVE_DIR, dataset_name)\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    num_shared_read = NUM_SHARED_READS[1] if 'R' in dataset_name else NUM_SHARED_READS[0]\n",
    "    is_deserialize = os.path.exists(os.path.join(processed_dir, dataset_name + '.json'))\n",
    "    n_clusters = n_clusters_mapping[dataset_name]\n",
    "    \n",
    "    # Read dataset\n",
    "    tqdm.tqdm.write(f'Processing dataset {dataset_name}...')\n",
    "    tqdm.tqdm.write(f'Prior number of clusters: {n_clusters}...')\n",
    "    tqdm.tqdm.write(f'Prior number of shared reads: {num_shared_read}...')\n",
    "\n",
    "    try:\n",
    "        seed_kmer_features, labels, groups, seeds = load_genomics(\n",
    "            dataset,\n",
    "            kmers=KMERS,\n",
    "            lmer=LMER,\n",
    "            maximum_seed_size=MAXIMUM_SEED_SIZE,\n",
    "            num_shared_reads=num_shared_read,\n",
    "            is_deserialize=is_deserialize,\n",
    "            is_serialize=~is_deserialize,\n",
    "            is_normalize=True,\n",
    "            only_seed=ONLY_SEED,\n",
    "            graph_file=os.path.join(processed_dir, dataset_name + '.json')\n",
    "        )\n",
    "    except:\n",
    "        seed_kmer_features, labels, groups, seeds = load_genomics(\n",
    "            dataset,\n",
    "            kmers=KMERS,\n",
    "            lmer=LMER,\n",
    "            maximum_seed_size=MAXIMUM_SEED_SIZE,\n",
    "            num_shared_reads=num_shared_read,\n",
    "            is_deserialize=False,\n",
    "            is_serialize=True,\n",
    "            is_normalize=True,\n",
    "            only_seed=ONLY_SEED,\n",
    "            graph_file=os.path.join(processed_dir, dataset_name + '.json')\n",
    "        )\n",
    "    \n",
    "    # continue\n",
    "    # Initialize model\n",
    "    init_lr = 0.1\n",
    "    init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                               distribution='uniform')  # [-limit, limit], limit=sqrt(1./fan_in)\n",
    "    pretrain_optimizer = SGD(lr=init_lr, momentum=0.9, decay=init_lr/PRETRAIN_EPOCHS)\n",
    "\n",
    "    dec = DEC(dims=[seed_kmer_features.shape[-1], 500, 1000, 10], n_clusters=n_clusters, init=init)\n",
    "    \n",
    "    # Start clustering\n",
    "    if AE_WEIGHTS is None:\n",
    "        dec.pretrain(x=seed_kmer_features, y=labels, grps=groups, optimizer=pretrain_optimizer,\n",
    "                epochs=PRETRAIN_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                save_dir=save_dir)\n",
    "    else:\n",
    "        dec.autoencoder.load_weights(AE_WEIGHTS)\n",
    "\n",
    "    dec.model.summary()\n",
    "    t0 = time.time()\n",
    "    optim_lr = 0.01\n",
    "\n",
    "    dec.compile(optimizer=SGD(optim_lr), loss='kld')\n",
    "    y_pred = dec.fit(x=seed_kmer_features, y=labels, grps=groups, n_clusters=n_clusters, tol=TOL, maxiter=MAX_ITERS, batch_size=BATCH_SIZE,\n",
    "                      update_interval=UPDATE_INTERVAL, save_dir=save_dir)\n",
    "    \n",
    "    tqdm.tqdm.write('...')\n",
    "    latent = dec.encoder.predict(seed_kmer_features)\n",
    "    y_pred = dec.predict(seed_kmer_features)\n",
    "\n",
    "    tqdm.tqdm.write('Saving results...')\n",
    "    store_results(groups, seed_kmer_features, latent, labels, y_pred,\n",
    "                  n_clusters, dataset_name, save_dir=os.path.join(LOG_DIR, dataset_name))\n",
    "    tqdm.tqdm.write(f'Finish clustering for dataset {dataset_name}.')\n",
    "    tqdm.tqdm.write(f'F1-score: {genome_acc(groups, y_pred, labels, n_clusters)[2]}')\n",
    "    tqdm.tqdm.write(f'Clustering time: (time.time() - t0)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
